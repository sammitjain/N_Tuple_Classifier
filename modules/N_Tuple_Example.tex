% This LaTeX was auto-generated from MATLAB code.
% To make changes, update the MATLAB code and export to LaTeX again.

\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{color}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage{matlab}

\sloppy
\epstopdfsetup{outdir=./}
\graphicspath{ {./N_Tuple_Example_images/} }

\begin{document}

\matlabtitle{N-Tuple Classifier for Devanagri Digits}

\matlabheading{Prepare data}

\begin{par}
\begin{flushleft}
Load the dataset into a variable called data. Here data is assumed to have records as rows and the feature values as columns. The final column is the target class in numerical form. In our example, this translates to having 20,000 rows and 1024+1 columns.
\end{flushleft}
\end{par}

\begin{matlabcode}
load deva_final.mat data;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Check if the data is loaded properly by printing out a section of the data along with it's attributes.
\end{flushleft}
\end{par}

\begin{matlabcode}
data_info = whos('data')
\end{matlabcode}
\begin{matlaboutput}
data_info = 
          name: 'data'
          size: [20000 1025]
         bytes: 164000000
         class: 'double'
        global: 0
        sparse: 0
       complex: 0
       nesting: [1x1 struct]
    persistent: 0

\end{matlaboutput}

\begin{par}
\begin{flushleft}
Printing out a 15 x 8 section from the dataset:
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(data(100:115,800:808));
\end{matlabcode}
\begin{matlaboutput}
     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     1     1     1     1
     0     0     0     0     0     0     1     1     1
     0     0     0     0     0     0     1     1     1
     0     0     0     0     0     0     0     1     1
     0     0     0     0     0     0     1     1     1
     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     1     1     1     1
     0     0     0     0     0     1     1     1     1
     0     0     0     0     0     0     1     1     1
     0     0     0     0     0     1     1     1     0
     0     0     0     0     0     1     1     1     1
     0     0     0     0     1     1     1     0     0
     0     0     0     0     1     1     1     0     0
     0     0     0     0     0     1     1     1     1
     0     0     0     0     0     0     0     1     1
\end{matlaboutput}

\begin{par}
\begin{flushleft}
Define the number of classes K.
\end{flushleft}
\end{par}

\begin{matlabcode}
K = 10;
\end{matlabcode}


\matlabheading{Quantize data into L levels: 0,1,...,L-1}

\begin{par}
\begin{flushleft}
Decide the number of levels L. As a simpler and more prevalent use-case for image-related sets, we assume that all features will be quantized into the same number of levels.
\end{flushleft}
\end{par}

\begin{matlabcode}
L = 2;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Choose the type of quantization. At the moment we go with simple quantization with equal divisions. Such a quantization has been assigned type '1'.
\end{flushleft}
\end{par}

\begin{matlabcode}
type = 1;
\end{matlabcode}

\begin{par}
\begin{flushleft}
In the event of equal divisions, the quantization algorithm needs the minimum and maximum feature values. In our case they are 0 and 255. These minimum and maximum values are passed as a vector into a variable d\_range.
\end{flushleft}
\end{par}

\begin{matlabcode}
d_range = [0 1];
\end{matlabcode}

\begin{par}
\begin{flushleft}
Then we call the quantization function to assign the correct quantized values corresponding to each entry in the data matrix.
\end{flushleft}
\end{par}

\begin{matlabcode}
[data,level_vals] = n_quantize(data,d_range,L,type);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Then we check the portion of data to see if it has been successfully quantized.
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(data(100:115,800:808));
\end{matlabcode}
\begin{matlaboutput}
     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     1     1     1     1
     0     0     0     0     0     0     1     1     1
     0     0     0     0     0     0     1     1     1
     0     0     0     0     0     0     0     1     1
     0     0     0     0     0     0     1     1     1
     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     1     1     1     1
     0     0     0     0     0     1     1     1     1
     0     0     0     0     0     0     1     1     1
     0     0     0     0     0     1     1     1     0
     0     0     0     0     0     1     1     1     1
     0     0     0     0     1     1     1     0     0
     0     0     0     0     1     1     1     0     0
     0     0     0     0     0     1     1     1     1
     0     0     0     0     0     0     0     1     1
\end{matlaboutput}

\begin{par}
\begin{flushleft}
Additionally the levels used by the quantization algorithm are stored in level\_vals.
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(level_vals);
\end{matlabcode}
\begin{matlaboutput}
     0     1     1
\end{matlaboutput}


\matlabheading{Train-Test-Crossval Splitting}

\begin{par}
\begin{flushleft}
As described earlier, typical workflow in most classification algorithms involves splitting the dataset into three portions. In the basic implementation we only use the training and testing sets.
\end{flushleft}
\end{par}

\begin{matlabcode}
p_train = 0.75;
p_test = 0.25;
p_crossval = 0;
[train,test,crossval] = n_ttc_split(data,p_train,p_test,p_crossval);
\end{matlabcode}

\begin{par}
\begin{flushleft}
We can check if the splitting was indeed according to the proportion specificed:
\end{flushleft}
\end{par}

\begin{matlabcode}
train_info = whos('train')
\end{matlabcode}
\begin{matlaboutput}
train_info = 
          name: 'train'
          size: [15000 1025]
         bytes: 123000000
         class: 'double'
        global: 0
        sparse: 0
       complex: 0
       nesting: [1x1 struct]
    persistent: 0

\end{matlaboutput}
\begin{matlabcode}
test_info = whos('test')
\end{matlabcode}
\begin{matlaboutput}
test_info = 
          name: 'test'
          size: [5000 1025]
         bytes: 41000000
         class: 'double'
        global: 0
        sparse: 0
       complex: 0
       nesting: [1x1 struct]
    persistent: 0

\end{matlaboutput}
\begin{matlabcode}
crossval_info = whos('crossval')
\end{matlabcode}
\begin{matlaboutput}
crossval_info = 
          name: 'crossval'
          size: [0 1025]
         bytes: 0
         class: 'double'
        global: 0
        sparse: 0
       complex: 0
       nesting: [1x1 struct]
    persistent: 0

\end{matlaboutput}


\matlabheading{Create tuples}

\begin{par}
\begin{flushleft}
Decide the length (N) of each tuple. In this example we assume that all tuples are of equal size. Although mathematically we are allowed to have tuples of varying sizes, we stick to uniform sizes for simplicity in implementation.
\end{flushleft}
\end{par}

\begin{matlabcode}
N = 10;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Decide the number of tuples.
\end{flushleft}
\end{par}

\begin{matlabcode}
T = 100;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Decide the number of features, or just let the algorithm figure out on it's own using the size of variable data.
\end{flushleft}
\end{par}

\begin{matlabcode}
F = size(data,2)-1
\end{matlabcode}
\begin{matlaboutput}
F = 1024
\end{matlaboutput}

\begin{par}
\begin{flushleft}
Create tuples using the previously declared variables.
\end{flushleft}
\end{par}

\begin{matlabcode}
tuples = n_make_tuples(N,T,F);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Check if the tuples have been created properly. 
\end{flushleft}
\end{par}

\begin{matlabcode}
tuples_info = whos('tuples')
\end{matlabcode}
\begin{matlaboutput}
tuples_info = 
          name: 'tuples'
          size: [100 10]
         bytes: 8000
         class: 'double'
        global: 0
        sparse: 0
       complex: 0
       nesting: [1x1 struct]
    persistent: 0

\end{matlaboutput}

\begin{par}
\begin{flushleft}
Print out a portion to check the values.
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(tuples(3:13,1:4));
\end{matlabcode}
\begin{matlaboutput}
   602   986   474   146
   904   400   656   585
   104    83   207   964
   950   529    33   235
   677   637    28   727
   170   377   372   507
   176   375    75   512
    12   593   304   821
   708    12   710   803
   259   174   800   727
   431   860    57   315
\end{matlaboutput}


\matlabheading{Create memory structure}

\begin{par}
\begin{flushleft}
To create a memory structure with all the K x T tables, as mentioned earlier we need to be careful of how the values in these table would be combined. The starting value for the table (val), if the product rule will be used, should be non-zero. We choose to go with 1. We will later how this value affects the results.
\end{flushleft}
\end{par}

\begin{matlabcode}
val = 1; 
\end{matlabcode}

\begin{par}
\begin{flushleft}
We create the memory structure using the previously supplied information. The variable mem is a 1x100 cell array, with each cell having 10 cells. Each of these 10 cells has 1024 values.
\end{flushleft}
\end{par}

\begin{matlabcode}
mem = n_create_memory(N,T,L,K,val); mem_info = whos('mem')
\end{matlabcode}
\begin{matlaboutput}
mem_info = 
          name: 'mem'
          size: [1 100]
         bytes: 8315200
         class: 'cell'
        global: 0
        sparse: 0
       complex: 0
       nesting: [1x1 struct]
    persistent: 0

\end{matlaboutput}

\begin{par}
\begin{flushleft}
Check values in mem. Printing out first ten values of the table corresponding to tuple t = 4, and class k = 8. They should be all asigned equal to the value specified in the variable val.
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(mem{4}{8}(1:10));
\end{matlabcode}
\begin{matlaboutput}
     1
     1
     1
     1
     1
     1
     1
     1
     1
     1
\end{matlaboutput}


\matlabheading{Tabulate probability values (Training)}

\begin{par}
\begin{flushleft}
The training function loops through all the observations and tuples to tabulate the values for our memory tables. The tic-toc function is used to keep track of the training time. 
\end{flushleft}
\end{par}

\begin{matlabcode}
tic; mem = n_tuple_train(train,tuples,mem,N,T,L,K); toc;
\end{matlabcode}
\begin{matlaboutput}
Tuple number: 1
Tuple number: 2
Tuple number: 3
Tuple number: 4
Tuple number: 5
Tuple number: 6
Tuple number: 7
Tuple number: 8
Tuple number: 9
Tuple number: 10
Tuple number: 11
Tuple number: 12
Tuple number: 13
Tuple number: 14
Tuple number: 15
Tuple number: 16
Tuple number: 17
Tuple number: 18
Tuple number: 19
Tuple number: 20
Tuple number: 21
Tuple number: 22
Tuple number: 23
Tuple number: 24
Tuple number: 25
Tuple number: 26
Tuple number: 27
Tuple number: 28
Tuple number: 29
Tuple number: 30
Tuple number: 31
Tuple number: 32
Tuple number: 33
Tuple number: 34
Tuple number: 35
Tuple number: 36
Tuple number: 37
Tuple number: 38
Tuple number: 39
Tuple number: 40
Tuple number: 41
Tuple number: 42
Tuple number: 43
Tuple number: 44
Tuple number: 45
Tuple number: 46
Tuple number: 47
Tuple number: 48
Tuple number: 49
Tuple number: 50
Tuple number: 51
Tuple number: 52
Tuple number: 53
Tuple number: 54
Tuple number: 55
Tuple number: 56
Tuple number: 57
Tuple number: 58
Tuple number: 59
Tuple number: 60
Tuple number: 61
Tuple number: 62
Tuple number: 63
Tuple number: 64
Tuple number: 65
Tuple number: 66
Tuple number: 67
Tuple number: 68
Tuple number: 69
Tuple number: 70
Tuple number: 71
Tuple number: 72
Tuple number: 73
Tuple number: 74
Tuple number: 75
Tuple number: 76
Tuple number: 77
Tuple number: 78
Tuple number: 79
Tuple number: 80
Tuple number: 81
Tuple number: 82
Tuple number: 83
Tuple number: 84
Tuple number: 85
Tuple number: 86
Tuple number: 87
Tuple number: 88
Tuple number: 89
Tuple number: 90
Tuple number: 91
Tuple number: 92
Tuple number: 93
Tuple number: 94
Tuple number: 95
Tuple number: 96
Tuple number: 97
Tuple number: 98
Tuple number: 99
Tuple number: 100
Elapsed time is 12.169074 seconds.
\end{matlaboutput}

\begin{par}
\begin{flushleft}
Check values in mem. Printing out first ten values of the table corresponding to tuple t = 4, and class k = 8. They should now reflect the conditional probabilities of finding the tuple t=4 in an observation belonging to class k = 8.
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(mem{4}{8}(1:10));
\end{matlabcode}
\begin{matlaboutput}
    0.0960
    0.0033
    0.0120
    0.0020
    0.0027
    0.0007
    0.0007
    0.0007
    0.0453
    0.0020
\end{matlaboutput}


\matlabheading{Adjust values, cross-validation, combining classifiers, etc}

\begin{par}
\begin{flushleft}
This part of the code will typically deal with modifying the original N-Tuple scheme using the cross-validation data. Since this is a basic implementation, we move on to the testing procedure directly for the time being.
\end{flushleft}
\end{par}

\matlabheading{Test the classifer}

\begin{par}
\begin{flushleft}
While testing the N-Tuple classifier, one of the most important decisions to be made is the strategy for combination of scores. We use the variable rule (as 1 or 2) to decide the combination rule. The sum rule with these values of N and T gives \textasciitilde{}82\% accuracy, while the product rule performs brilliantly with the Devanagri set \textasciitilde{}92\% accuracy.
\end{flushleft}
\end{par}

\begin{matlabcode}
rule = 2;
\end{matlabcode}

\begin{par}
\begin{flushleft}
Calculate the scores for the test set.
\end{flushleft}
\end{par}

\begin{matlabcode}
test_scores = n_tuple_test(test,tuples,mem,rule,N,T,L,K);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Check the test\_scores for the first observation in the test set.
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(test_scores(1,:)');
\end{matlabcode}
\begin{matlaboutput}
  1.0e-153 *

    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.1371
\end{matlaboutput}

\begin{par}
\begin{flushleft}
Predict the final class of the test data using a majority vote approach.
\end{flushleft}
\end{par}

\begin{matlabcode}
predictions = zeros(size(test,1),1);
    for m = 1:size(test,1)
        [~,predictions(m)] = max(test_scores(m,:));
    end
\end{matlabcode}

\begin{par}
\begin{flushleft}
Let us check the predictions for the first 5 obervations in the test set.
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(predictions(1:5));
\end{matlabcode}
\begin{matlaboutput}
    10
     2
     8
     9
     6
\end{matlaboutput}

\begin{par}
\begin{flushleft}
Using the predictions for the testing data, we calculate the accuracy of our N-Tuple classifier.
\end{flushleft}
\end{par}

\begin{matlabcode}
accuracy = 100*sum(predictions==test(:,end))/size(test,1);
\end{matlabcode}

\begin{par}
\begin{flushleft}
Printing the accuracy:
\end{flushleft}
\end{par}

\begin{matlabcode}
disp(['Accuracy = ' num2str(accuracy) ' %']);
\end{matlabcode}
\begin{matlaboutput}
Accuracy = 93.72 %
\end{matlaboutput}

\end{document}
